{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b8db90-0e00-4b3a-845f-b042c68d865b",
   "metadata": {},
   "source": [
    "# Task 5\n",
    "# BERT Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33605e5a-4428-444e-b386-131bcc8a7c5b",
   "metadata": {},
   "source": [
    "## 5.1 Base BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f8e9dd-4f61-4b5d-beee-dc6076375a15",
   "metadata": {},
   "source": [
    "Pre-training followed by fine-tuning is a kind of transfer learning - learning knowledge from one task, and applying it to thiese SDMH classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4d9818-588b-41db-b1ba-dfe06e9b9088",
   "metadata": {},
   "source": [
    "### 5.1.1 Import Packages & Setup\n",
    "First, I installed the  TensorFlow library to enable building and fine-tuning a Hugging Face model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fd14b0-5594-4e3f-814b-b735bb9c85f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertPreTrainedModel\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "# Read in three datasets again\n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_val = pd.read_csv('data_val.csv')\n",
    "data_test = pd.read_csv('data_test.csv')\n",
    "\n",
    "# For build\n",
    "# data_train = data_train.sample(frac=0.003, random_state=42).reset_index(drop=True)\n",
    "# data_val = data_val.sample(frac=0.05, random_state=42).reset_index(drop=True)\n",
    "# data_test = data_test.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
    "\n",
    "full_cols= [\n",
    "    'sdoh_community_present', 'sdoh_community_absent', 'sdoh_education',\n",
    "    'sdoh_economics', 'sdoh_environment', 'behavior_alcohol',\n",
    "    'behavior_tobacco', 'behavior_drug'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39bbe7-e27c-4fad-b1e6-8b7dfd8d95c5",
   "metadata": {},
   "source": [
    "#### Convert to Hugging Face Dataset\n",
    "Using the Dataset makes it easier to handle data for using Hugging Face training tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2d3234-0365-4282-b26d-14dce1f7ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Hugging Face Dataset\n",
    "train_ds=Dataset.from_pandas(data_train)\n",
    "val_ds=Dataset.from_pandas(data_val)\n",
    "test_ds=Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ddf205-ff99-44e5-8a81-4b1b51015dd7",
   "metadata": {},
   "source": [
    "#### Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37afe08-c1d4-4205-ab35-72ba151bfc7c",
   "metadata": {},
   "source": [
    "##### Tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdabd3f-1bef-4d9a-ab6a-4d40aeb027f5",
   "metadata": {},
   "source": [
    "start by loading the BERT Base cased pretrained tokenisation model form Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c184fb5-c8e8-47fc-8679-7c23f326dfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28d036c-30b6-4351-ab90-e476fe4e69be",
   "metadata": {},
   "source": [
    "##### Have a try on some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff62f73c-a4b7-499d-b275-48bf66494831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1996, 5776, 2387, 1037, 4248, 2829, 4419, 5598, 2058, 1996, 13971, 3899, 102]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode('the patient saw a quick brown fox jumped over the lazy dog')\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92e6b54-bf7b-4b8d-8ae9-323eab87958b",
   "metadata": {},
   "source": [
    "##### define two simple functions that takes a batch of text and combined labels as input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d7c7c-ec37-4108-8037-894dd5119b73",
   "metadata": {},
   "source": [
    "Define a simple tokenisation function that takes a batch of text and labels as input, takes out the text part of it, and returns the tokenised text and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da4ca27c-6e8f-4d80-a2d2-38cbda4ee28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1df825fa96d4251a93589d2b968b6a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "689e30ce85094626b6e98f31ddf2b4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178e2f8790d144d0b5995a78c2290fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c984292e994e088d96b078c7955567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad06baa66194ecc861e808d42b42768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b08ecd85de64442c8104b27399943417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenisation function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['social_history'], padding='max_length',truncation=True,max_length=128)\n",
    "    \n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Combine labels function\n",
    "def combine(batch):\n",
    "    labels = []\n",
    "    for i in range(len(batch[full_cols[0]])):\n",
    "        row=[]\n",
    "        for cols in full_cols:\n",
    "            row.append(batch[cols][i])\n",
    "        labels.append(row)\n",
    "    batch['labels']= labels\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(combine, batched=True)\n",
    "val_ds = val_ds.map(combine, batched=True)\n",
    "test_ds = test_ds.map(combine, batched=True)\n",
    "\n",
    "\n",
    "# Remove extra columns to avoid passing extra original 8 labels' columns to model\n",
    "# Use remove rather than select\n",
    "use_cols=['input_ids', 'attention_mask', 'labels']\n",
    "train_ds=train_ds.remove_columns([cols for cols in train_ds.column_names if cols not in use_cols])\n",
    "val_ds=val_ds.remove_columns([cols for cols in val_ds.column_names if cols not in use_cols])\n",
    "test_ds= test_ds.remove_columns([cols for cols in test_ds.column_names if cols not in use_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7f88a9-6b76-4813-8fa5-a4ac89a6fac9",
   "metadata": {},
   "source": [
    "### 5.1.2 Create the Standard BERT Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f044b4a-cced-4a5e-855e-ca1e04e04439",
   "metadata": {},
   "source": [
    "Similarly, I build a model that shares a single encoder and uses a separate classification head (nn.Linear) for each task\n",
    "- Merges logits from all tasks into a unified tensor with padding for alignment\n",
    "- Supports joint loss computation across tasks using CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aa1b118-dd72-4bf1-8556-c0ec6f10fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight', 'classifiers.6.bias', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.7.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "labelclass_list=[2,2,2,3,3,5,5,5]\n",
    "\n",
    "# Use the pretrained BertPreTrainedModel class for initialization\n",
    "class MultiTaskBertModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.labelclass_list = labelclass_list\n",
    "        self.num_tasks = 8\n",
    "        \n",
    "        self.bert=AutoModel.from_pretrained('bert-base-uncased', config=config)\n",
    "\n",
    "        # Add a Hidden layer before classification heads\n",
    "        #self.hidden = nn.Sequential(\n",
    "            #nn.Linear(config.hidden_size, 256),\n",
    "            #nn.GELU(),\n",
    "            # Add a 50% dropout\n",
    "            #nn.Dropout(0.5)\n",
    "        #)\n",
    "        \n",
    "        # Use one classification head per task and register them using nn.ModuleList.\n",
    "        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, num_labels) for num_labels in labelclass_list])\n",
    "        self.init_weights()\n",
    "\n",
    "        \n",
    "    # Define forward pass for multi-task model to compute combined loss for joint training\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        # hidden_output = self.hidden(pooled_output)\n",
    "        logits_list = [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        batch_size = pooled_output.size(0)\n",
    "        device = pooled_output.device\n",
    "\n",
    "        # As Using Trainer needs unified logits with consistent shape across tasks. Combine logits from 8 tasks and use 0 as padding to mask unused positions.\n",
    "        # maximun of num_labels is 5\n",
    "\n",
    "        # Initialize logits with zeros, shape: (batch_size, num_tasks, max_labels)\n",
    "        logits = torch.zeros(batch_size, self.num_tasks, 5,device=device)\n",
    "\n",
    "        for i, logit_task in enumerate(logits_list):\n",
    "\n",
    "            num_labels = self.labelclass_list[i]\n",
    "            #print(logit_task.shape)\n",
    "            logits[:, i, :num_labels] = logit_task\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        for i in range(self.num_tasks):\n",
    "            loss += loss_fct(logits[:, i, :self.labelclass_list[i]], labels[:, i])\n",
    "\n",
    "        return (loss, logits)\n",
    "\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased')\n",
    "model = MultiTaskBertModel.from_pretrained('bert-base-uncased',config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761116f8-9ccf-43da-b0cc-ae7623cbcb4d",
   "metadata": {},
   "source": [
    "### 5.1.3 Set Up Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad53a5f3-b72c-4b61-b7b4-cebe3157c4e5",
   "metadata": {},
   "source": [
    "Same as other basic models above, macro and weighted scores are chosen to evaluate model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd8521f8-6340-4b8d-8c7c-ff84be701c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits,labels=eval_pred\n",
    "    # labels = labels.astype(int)\n",
    "\n",
    "    # Initialize lists\n",
    "    precision_macro=[]\n",
    "    recall_macro = []\n",
    "    f1_macro=[]\n",
    "\n",
    "    precision_weighted=[]\n",
    "    recall_weighted=[]\n",
    "    f1_weighted=[]\n",
    "\n",
    "    acclist=[]\n",
    "    task_f1_scores={}\n",
    "\n",
    "    for i in range(8):\n",
    "        logit_task = logits[:, i, :labelclass_list[i]]\n",
    "        preds_i = np.argmax(logit_task, axis=1)\n",
    "        y_true = labels[:, i]\n",
    "\n",
    "        acc = accuracy_score(y_true, preds_i)\n",
    "        acclist.append(acc)\n",
    "\n",
    "        # macro\n",
    "        prec=precision_score(y_true, preds_i, average='macro')\n",
    "        rec=recall_score(y_true, preds_i, average='macro')\n",
    "        f1=f1_score(y_true, preds_i, average='macro')\n",
    "        precision_macro.append(prec)\n",
    "        recall_macro.append(rec)\n",
    "        f1_macro.append(f1)\n",
    "        \n",
    "        # As it's not convinent to directly compute the macro F1 score for each task category on the test dataset,\n",
    "        # the macro F1 score at each epoch was calculated\n",
    "        task_f1_scores[full_cols[i]+\"_macro_f1\"] = f1\n",
    "\n",
    "        # weighted\n",
    "        prec_w=precision_score(y_true, preds_i, average='weighted')\n",
    "        rec_w=recall_score(y_true, preds_i, average='weighted')\n",
    "        f1_w=f1_score(y_true, preds_i, average='weighted')\n",
    "        precision_weighted.append(prec_w)\n",
    "        recall_weighted.append(rec_w)\n",
    "        f1_weighted.append(f1_w)\n",
    "\n",
    "    metrics = {\n",
    "        'macro_acc': np.mean(acclist),\n",
    "\n",
    "        'macro_precision': np.mean(precision_macro),\n",
    "        'macro_recall': np.mean(recall_macro),\n",
    "        'macro_f1': np.mean(f1_macro),\n",
    "\n",
    "        'weighted_precision': np.mean(precision_weighted),\n",
    "        'weighted_recall': np.mean(recall_weighted),\n",
    "        'weighted_f1': np.mean(f1_weighted),\n",
    "    }\n",
    "\n",
    "    metrics.update(task_f1_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9421ad-e2d2-4d15-a53a-3b9508179dc4",
   "metadata": {},
   "source": [
    "### 5.1.4 Set up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a539cd-3d81-4826-8f43-699fec695589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a few arguments in a seperate TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir ='./Final_Project',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size = 16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    #15 epochs to reach relatively stable\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',    \n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    #report_to=\"tensorboard\",\n",
    "    #Evaluate after each epoch and save checkpoints to easily resume training, add epochs\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "\n",
    "    # Set to automatically load the the best macro_f1 score model on validation set\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    processing_class= tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322255fa-139d-49a8-8110-ce1c10d968ce",
   "metadata": {},
   "source": [
    "### 5.1.5 Train the Module & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1660f643-2b6a-45ee-b581-39243939bc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4620' max='4620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4620/4620 4:18:52, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro Acc</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted Precision</th>\n",
       "      <th>Weighted Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Sdoh Community Present Macro F1</th>\n",
       "      <th>Sdoh Community Absent Macro F1</th>\n",
       "      <th>Sdoh Education Macro F1</th>\n",
       "      <th>Sdoh Economics Macro F1</th>\n",
       "      <th>Sdoh Environment Macro F1</th>\n",
       "      <th>Behavior Alcohol Macro F1</th>\n",
       "      <th>Behavior Tobacco Macro F1</th>\n",
       "      <th>Behavior Drug Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.910400</td>\n",
       "      <td>3.701662</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.655736</td>\n",
       "      <td>0.578165</td>\n",
       "      <td>0.566819</td>\n",
       "      <td>0.828689</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.823307</td>\n",
       "      <td>0.910095</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.550012</td>\n",
       "      <td>0.607533</td>\n",
       "      <td>0.475393</td>\n",
       "      <td>0.562691</td>\n",
       "      <td>0.379181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.514600</td>\n",
       "      <td>2.322437</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.806499</td>\n",
       "      <td>0.718903</td>\n",
       "      <td>0.740230</td>\n",
       "      <td>0.910222</td>\n",
       "      <td>0.916825</td>\n",
       "      <td>0.909670</td>\n",
       "      <td>0.973265</td>\n",
       "      <td>0.876270</td>\n",
       "      <td>0.647328</td>\n",
       "      <td>0.838419</td>\n",
       "      <td>0.632662</td>\n",
       "      <td>0.670736</td>\n",
       "      <td>0.778025</td>\n",
       "      <td>0.505133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.664400</td>\n",
       "      <td>1.758617</td>\n",
       "      <td>0.936611</td>\n",
       "      <td>0.852275</td>\n",
       "      <td>0.786118</td>\n",
       "      <td>0.804176</td>\n",
       "      <td>0.933339</td>\n",
       "      <td>0.936611</td>\n",
       "      <td>0.932459</td>\n",
       "      <td>0.979435</td>\n",
       "      <td>0.944930</td>\n",
       "      <td>0.828956</td>\n",
       "      <td>0.892471</td>\n",
       "      <td>0.639514</td>\n",
       "      <td>0.770375</td>\n",
       "      <td>0.826875</td>\n",
       "      <td>0.550853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.328400</td>\n",
       "      <td>1.532853</td>\n",
       "      <td>0.942417</td>\n",
       "      <td>0.887173</td>\n",
       "      <td>0.815248</td>\n",
       "      <td>0.832272</td>\n",
       "      <td>0.940539</td>\n",
       "      <td>0.942417</td>\n",
       "      <td>0.940114</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.950455</td>\n",
       "      <td>0.864768</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.704612</td>\n",
       "      <td>0.821946</td>\n",
       "      <td>0.852799</td>\n",
       "      <td>0.599649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.953300</td>\n",
       "      <td>1.396813</td>\n",
       "      <td>0.947986</td>\n",
       "      <td>0.897378</td>\n",
       "      <td>0.838233</td>\n",
       "      <td>0.854001</td>\n",
       "      <td>0.946808</td>\n",
       "      <td>0.947986</td>\n",
       "      <td>0.946342</td>\n",
       "      <td>0.981575</td>\n",
       "      <td>0.946323</td>\n",
       "      <td>0.903317</td>\n",
       "      <td>0.890066</td>\n",
       "      <td>0.757534</td>\n",
       "      <td>0.842351</td>\n",
       "      <td>0.877696</td>\n",
       "      <td>0.633145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.782600</td>\n",
       "      <td>1.305240</td>\n",
       "      <td>0.950474</td>\n",
       "      <td>0.897880</td>\n",
       "      <td>0.842770</td>\n",
       "      <td>0.856179</td>\n",
       "      <td>0.949265</td>\n",
       "      <td>0.950474</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.902314</td>\n",
       "      <td>0.758244</td>\n",
       "      <td>0.846654</td>\n",
       "      <td>0.887983</td>\n",
       "      <td>0.635515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.296835</td>\n",
       "      <td>0.950948</td>\n",
       "      <td>0.893783</td>\n",
       "      <td>0.846942</td>\n",
       "      <td>0.860911</td>\n",
       "      <td>0.949855</td>\n",
       "      <td>0.950948</td>\n",
       "      <td>0.949735</td>\n",
       "      <td>0.977455</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.877288</td>\n",
       "      <td>0.893195</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.862729</td>\n",
       "      <td>0.886555</td>\n",
       "      <td>0.638828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.498600</td>\n",
       "      <td>1.266390</td>\n",
       "      <td>0.954028</td>\n",
       "      <td>0.913661</td>\n",
       "      <td>0.858461</td>\n",
       "      <td>0.875488</td>\n",
       "      <td>0.953555</td>\n",
       "      <td>0.954028</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>0.979482</td>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.904098</td>\n",
       "      <td>0.912758</td>\n",
       "      <td>0.798439</td>\n",
       "      <td>0.875257</td>\n",
       "      <td>0.881556</td>\n",
       "      <td>0.695573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>1.247788</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.910708</td>\n",
       "      <td>0.874528</td>\n",
       "      <td>0.886149</td>\n",
       "      <td>0.955211</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.954997</td>\n",
       "      <td>0.978419</td>\n",
       "      <td>0.952641</td>\n",
       "      <td>0.906155</td>\n",
       "      <td>0.903038</td>\n",
       "      <td>0.839612</td>\n",
       "      <td>0.870337</td>\n",
       "      <td>0.897282</td>\n",
       "      <td>0.741712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>1.236513</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.910801</td>\n",
       "      <td>0.881029</td>\n",
       "      <td>0.892894</td>\n",
       "      <td>0.954464</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.954207</td>\n",
       "      <td>0.975378</td>\n",
       "      <td>0.952641</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.905183</td>\n",
       "      <td>0.898923</td>\n",
       "      <td>0.865713</td>\n",
       "      <td>0.897208</td>\n",
       "      <td>0.754459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>1.248393</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.913928</td>\n",
       "      <td>0.877743</td>\n",
       "      <td>0.891187</td>\n",
       "      <td>0.955568</td>\n",
       "      <td>0.955687</td>\n",
       "      <td>0.955222</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.957062</td>\n",
       "      <td>0.907152</td>\n",
       "      <td>0.906407</td>\n",
       "      <td>0.872431</td>\n",
       "      <td>0.866406</td>\n",
       "      <td>0.895592</td>\n",
       "      <td>0.747098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>1.277964</td>\n",
       "      <td>0.956517</td>\n",
       "      <td>0.913204</td>\n",
       "      <td>0.891234</td>\n",
       "      <td>0.900388</td>\n",
       "      <td>0.956450</td>\n",
       "      <td>0.956517</td>\n",
       "      <td>0.956182</td>\n",
       "      <td>0.976363</td>\n",
       "      <td>0.950455</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.911529</td>\n",
       "      <td>0.923866</td>\n",
       "      <td>0.871197</td>\n",
       "      <td>0.902112</td>\n",
       "      <td>0.757569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>1.261687</td>\n",
       "      <td>0.956754</td>\n",
       "      <td>0.911722</td>\n",
       "      <td>0.889182</td>\n",
       "      <td>0.898522</td>\n",
       "      <td>0.956728</td>\n",
       "      <td>0.956754</td>\n",
       "      <td>0.956467</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>0.954844</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.908711</td>\n",
       "      <td>0.925214</td>\n",
       "      <td>0.871287</td>\n",
       "      <td>0.906379</td>\n",
       "      <td>0.752741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.172800</td>\n",
       "      <td>1.250808</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.915218</td>\n",
       "      <td>0.891431</td>\n",
       "      <td>0.901257</td>\n",
       "      <td>0.957331</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.957033</td>\n",
       "      <td>0.975378</td>\n",
       "      <td>0.959296</td>\n",
       "      <td>0.910012</td>\n",
       "      <td>0.909961</td>\n",
       "      <td>0.924540</td>\n",
       "      <td>0.872323</td>\n",
       "      <td>0.905905</td>\n",
       "      <td>0.752640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.186700</td>\n",
       "      <td>1.260288</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.918145</td>\n",
       "      <td>0.886387</td>\n",
       "      <td>0.899222</td>\n",
       "      <td>0.957124</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.956905</td>\n",
       "      <td>0.976390</td>\n",
       "      <td>0.959296</td>\n",
       "      <td>0.916913</td>\n",
       "      <td>0.908711</td>\n",
       "      <td>0.900288</td>\n",
       "      <td>0.871872</td>\n",
       "      <td>0.904009</td>\n",
       "      <td>0.756296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model Performance on test dataset: \n",
      "{'eval_loss': 1.2566423416137695, 'eval_macro_acc': 0.9586894586894587, 'eval_macro_precision': 0.9098005480315166, 'eval_macro_recall': 0.8891243667621189, 'eval_macro_f1': 0.8976103070831851, 'eval_weighted_precision': 0.9584484113093603, 'eval_weighted_recall': 0.9586894586894587, 'eval_weighted_f1': 0.9583890595215988, 'eval_sdoh_community_present_macro_f1': 0.9824086949258307, 'eval_sdoh_community_absent_macro_f1': 0.941429498470381, 'eval_sdoh_education_macro_f1': 0.8968152866242038, 'eval_sdoh_economics_macro_f1': 0.9124918716761746, 'eval_sdoh_environment_macro_f1': 0.9162276768414875, 'eval_behavior_alcohol_macro_f1': 0.8556693389993264, 'eval_behavior_tobacco_macro_f1': 0.9245133722814941, 'eval_behavior_drug_macro_f1': 0.7513267168465827, 'eval_runtime': 54.9432, 'eval_samples_per_second': 19.165, 'eval_steps_per_second': 1.201, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "test_metrics = trainer.evaluate(test_ds)\n",
    "print(\"Best model Performance on test dataset: \")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aca7d8-27d5-459d-84e5-907edd75b79a",
   "metadata": {},
   "source": [
    "### 5.2 Bio-Clinical BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc51de6f-92b1-4aba-a7b6-00ac7ac0045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Bio-ClinicalBERT as model 2\n",
    "modelname2='emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_ClinicalBERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f115d62-9e15-4d04-ad0e-fd586f642368",
   "metadata": {},
   "source": [
    "##### Convert these three datasets to Hugging Face Dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77e37ffe-a546-4887-97f8-b4e029697f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_pandas(data_train)\n",
    "val_ds = Dataset.from_pandas(data_val)\n",
    "test_ds = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04f24d-80b5-49fa-8133-1d34ae513b0a",
   "metadata": {},
   "source": [
    "### 5.2.1 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c83aea47-fdcd-458c-8b84-a69fec637e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063e719e290849b2abd19abb48e4db90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b1db6e5c1b46c9bacaf2b0b03b3f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f866efd0e824e99a51b8a5f33e847b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e2550ed938483396e0814d6086f38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5281f4c69c48748c40c5fb7985fb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38d7efb7bf047698627d44cb389d55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1053 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same as above, use two functions to take a batch of text and combined labels as input\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['social_history'], padding='max_length',truncation=True,max_length=128)\n",
    "    \n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "def combine(batch):\n",
    "    labels = []\n",
    "    for i in range(len(batch[full_cols[0]])):\n",
    "        row =[]\n",
    "        for cols in full_cols:\n",
    "            row.append(batch[cols][i])\n",
    "        labels.append(row)\n",
    "    batch['labels'] = labels\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(combine, batched=True)\n",
    "val_ds = val_ds.map(combine, batched=True)\n",
    "test_ds = test_ds.map(combine, batched=True)\n",
    "\n",
    "\n",
    "# Remove extra columns to avoid passing original labels' columns to model\n",
    "use_cols=['input_ids', 'attention_mask', 'labels']\n",
    "train_ds = train_ds.remove_columns([cols for cols in train_ds.column_names if cols not in use_cols])\n",
    "val_ds = val_ds.remove_columns([cols for cols in val_ds.column_names if cols not in use_cols])\n",
    "test_ds = test_ds.remove_columns([cols for cols in test_ds.column_names if cols not in use_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c6e87-c9e8-4975-8fed-b2eaad099fb0",
   "metadata": {},
   "source": [
    "### 5.2.2 Create the BioClinicalBERT Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7458200e-b640-4e5f-b94d-3f804750018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MultiTaskBertModel2 were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifiers.0.bias', 'classifiers.0.weight', 'classifiers.1.bias', 'classifiers.1.weight', 'classifiers.2.bias', 'classifiers.2.weight', 'classifiers.3.bias', 'classifiers.3.weight', 'classifiers.4.bias', 'classifiers.4.weight', 'classifiers.5.bias', 'classifiers.5.weight', 'classifiers.6.bias', 'classifiers.6.weight', 'classifiers.7.bias', 'classifiers.7.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskBertModel2(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_tasks = 8\n",
    "        # self.bert = BertModel(config) \n",
    "        self.bert = AutoModel.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', config=config)  # Use Bio+Clinical BERT from huggingface\n",
    "        \n",
    "        self.classifiers = nn.ModuleList([nn.Linear(config.hidden_size, num_labels) for num_labels in labelclass_list])\n",
    "        self.labelclass_list = labelclass_list\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output=outputs.pooler_output\n",
    "\n",
    "        logits_list= [classifier(pooled_output) for classifier in self.classifiers]\n",
    "\n",
    "        batch_size = pooled_output.size(0)\n",
    "        device = pooled_output.device\n",
    "\n",
    "        # Initialize logits with zeros, shape: (batch_size, num_tasks, max_labels)\n",
    "        logits = torch.zeros(batch_size, self.num_tasks, 5, device=device)\n",
    "        for i,logit_task in enumerate(logits_list):\n",
    "            num_labels = self.labelclass_list[i]\n",
    "            logits[:, i, :num_labels] = logit_task\n",
    "\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        for i in range(self.num_tasks):\n",
    "            loss += loss_fct(logits[:, i, :self.labelclass_list[i]],labels[:, i])\n",
    "\n",
    "        return (loss, logits)\n",
    "\n",
    "config=AutoConfig.from_pretrained(modelname2)\n",
    "model=MultiTaskBertModel2.from_pretrained(modelname2, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17efeae3-241e-4976-b828-79ac8d072ad3",
   "metadata": {},
   "source": [
    "#### 5.2.3Set up training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c573a1-7cad-4777-a9ff-8cc2151f6f7d",
   "metadata": {},
   "source": [
    "Adam optimiser was chosen to be used to optimise multi-task classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8ac7e73-ec2d-4d21-8872-e589900a15c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a few arguments in a seperate TrainingArguments class\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./Final_Project2',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='steps',    \n",
    "    logging_steps=50,\n",
    "    push_to_hub=False,\n",
    "    #report_to=\"tensorboard\",\n",
    "    #Evaluate after each epoch and save checkpoints to easily resume training, add epochs\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "\n",
    "    # Set to automatically load the the best macro_f1 score model on validation set\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='macro_f1',\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer2 = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # tokenizer=tokenizer,\n",
    "    processing_class= tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce727971-bbf9-42f7-9443-c96c21ef7938",
   "metadata": {},
   "source": [
    "#### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2634fe48-dde6-488b-b5f3-564e2cb41ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4620' max='4620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4620/4620 7:21:37, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro Acc</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted Precision</th>\n",
       "      <th>Weighted Recall</th>\n",
       "      <th>Weighted F1</th>\n",
       "      <th>Sdoh Community Present Macro F1</th>\n",
       "      <th>Sdoh Community Absent Macro F1</th>\n",
       "      <th>Sdoh Education Macro F1</th>\n",
       "      <th>Sdoh Economics Macro F1</th>\n",
       "      <th>Sdoh Environment Macro F1</th>\n",
       "      <th>Behavior Alcohol Macro F1</th>\n",
       "      <th>Behavior Tobacco Macro F1</th>\n",
       "      <th>Behavior Drug Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.733800</td>\n",
       "      <td>3.480148</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.583767</td>\n",
       "      <td>0.588099</td>\n",
       "      <td>0.576620</td>\n",
       "      <td>0.822185</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.838668</td>\n",
       "      <td>0.885264</td>\n",
       "      <td>0.470647</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.695792</td>\n",
       "      <td>0.613403</td>\n",
       "      <td>0.477653</td>\n",
       "      <td>0.603042</td>\n",
       "      <td>0.374857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.524200</td>\n",
       "      <td>2.347675</td>\n",
       "      <td>0.913744</td>\n",
       "      <td>0.830754</td>\n",
       "      <td>0.682979</td>\n",
       "      <td>0.706172</td>\n",
       "      <td>0.909429</td>\n",
       "      <td>0.913744</td>\n",
       "      <td>0.901270</td>\n",
       "      <td>0.968105</td>\n",
       "      <td>0.716197</td>\n",
       "      <td>0.651621</td>\n",
       "      <td>0.846188</td>\n",
       "      <td>0.642113</td>\n",
       "      <td>0.599974</td>\n",
       "      <td>0.757656</td>\n",
       "      <td>0.467522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.677500</td>\n",
       "      <td>1.763966</td>\n",
       "      <td>0.934953</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>0.760982</td>\n",
       "      <td>0.787933</td>\n",
       "      <td>0.930510</td>\n",
       "      <td>0.934953</td>\n",
       "      <td>0.929175</td>\n",
       "      <td>0.970094</td>\n",
       "      <td>0.911619</td>\n",
       "      <td>0.777764</td>\n",
       "      <td>0.896979</td>\n",
       "      <td>0.643527</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.846764</td>\n",
       "      <td>0.572679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.388700</td>\n",
       "      <td>1.480179</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.860823</td>\n",
       "      <td>0.812089</td>\n",
       "      <td>0.830907</td>\n",
       "      <td>0.942533</td>\n",
       "      <td>0.945142</td>\n",
       "      <td>0.942727</td>\n",
       "      <td>0.981533</td>\n",
       "      <td>0.936312</td>\n",
       "      <td>0.841318</td>\n",
       "      <td>0.891117</td>\n",
       "      <td>0.643623</td>\n",
       "      <td>0.806188</td>\n",
       "      <td>0.868439</td>\n",
       "      <td>0.678723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.942500</td>\n",
       "      <td>1.373281</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.882330</td>\n",
       "      <td>0.845580</td>\n",
       "      <td>0.858694</td>\n",
       "      <td>0.948452</td>\n",
       "      <td>0.949526</td>\n",
       "      <td>0.948355</td>\n",
       "      <td>0.982509</td>\n",
       "      <td>0.944517</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.906433</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>0.831188</td>\n",
       "      <td>0.869776</td>\n",
       "      <td>0.708479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.803500</td>\n",
       "      <td>1.277832</td>\n",
       "      <td>0.952725</td>\n",
       "      <td>0.912232</td>\n",
       "      <td>0.838079</td>\n",
       "      <td>0.855107</td>\n",
       "      <td>0.951953</td>\n",
       "      <td>0.952725</td>\n",
       "      <td>0.951129</td>\n",
       "      <td>0.977299</td>\n",
       "      <td>0.947128</td>\n",
       "      <td>0.846737</td>\n",
       "      <td>0.909935</td>\n",
       "      <td>0.709488</td>\n",
       "      <td>0.860093</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>0.707727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.582100</td>\n",
       "      <td>1.227464</td>\n",
       "      <td>0.953910</td>\n",
       "      <td>0.907465</td>\n",
       "      <td>0.853427</td>\n",
       "      <td>0.867909</td>\n",
       "      <td>0.953077</td>\n",
       "      <td>0.953910</td>\n",
       "      <td>0.952794</td>\n",
       "      <td>0.982530</td>\n",
       "      <td>0.949342</td>\n",
       "      <td>0.893650</td>\n",
       "      <td>0.904754</td>\n",
       "      <td>0.759702</td>\n",
       "      <td>0.854005</td>\n",
       "      <td>0.879714</td>\n",
       "      <td>0.719575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.534100</td>\n",
       "      <td>1.235442</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.916702</td>\n",
       "      <td>0.846153</td>\n",
       "      <td>0.862540</td>\n",
       "      <td>0.953465</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.952585</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.958680</td>\n",
       "      <td>0.889936</td>\n",
       "      <td>0.900669</td>\n",
       "      <td>0.706680</td>\n",
       "      <td>0.865426</td>\n",
       "      <td>0.887577</td>\n",
       "      <td>0.714001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.388600</td>\n",
       "      <td>1.225310</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.924391</td>\n",
       "      <td>0.865781</td>\n",
       "      <td>0.885331</td>\n",
       "      <td>0.955921</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.955541</td>\n",
       "      <td>0.976253</td>\n",
       "      <td>0.954166</td>\n",
       "      <td>0.911356</td>\n",
       "      <td>0.912837</td>\n",
       "      <td>0.802855</td>\n",
       "      <td>0.863520</td>\n",
       "      <td>0.896223</td>\n",
       "      <td>0.765433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.348700</td>\n",
       "      <td>1.206875</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.919542</td>\n",
       "      <td>0.865574</td>\n",
       "      <td>0.883303</td>\n",
       "      <td>0.956306</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.956113</td>\n",
       "      <td>0.979458</td>\n",
       "      <td>0.956741</td>\n",
       "      <td>0.893628</td>\n",
       "      <td>0.908801</td>\n",
       "      <td>0.802212</td>\n",
       "      <td>0.861524</td>\n",
       "      <td>0.899059</td>\n",
       "      <td>0.765003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.256100</td>\n",
       "      <td>1.263657</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.905218</td>\n",
       "      <td>0.859731</td>\n",
       "      <td>0.870430</td>\n",
       "      <td>0.954302</td>\n",
       "      <td>0.954621</td>\n",
       "      <td>0.953786</td>\n",
       "      <td>0.976281</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>0.880948</td>\n",
       "      <td>0.907523</td>\n",
       "      <td>0.760913</td>\n",
       "      <td>0.863376</td>\n",
       "      <td>0.886968</td>\n",
       "      <td>0.735142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>1.244344</td>\n",
       "      <td>0.955569</td>\n",
       "      <td>0.911384</td>\n",
       "      <td>0.867722</td>\n",
       "      <td>0.880998</td>\n",
       "      <td>0.955338</td>\n",
       "      <td>0.955569</td>\n",
       "      <td>0.954976</td>\n",
       "      <td>0.976309</td>\n",
       "      <td>0.951934</td>\n",
       "      <td>0.883679</td>\n",
       "      <td>0.908386</td>\n",
       "      <td>0.803423</td>\n",
       "      <td>0.852944</td>\n",
       "      <td>0.900458</td>\n",
       "      <td>0.770849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>1.240165</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.917461</td>\n",
       "      <td>0.870818</td>\n",
       "      <td>0.884989</td>\n",
       "      <td>0.957079</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.956669</td>\n",
       "      <td>0.979435</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.906525</td>\n",
       "      <td>0.804065</td>\n",
       "      <td>0.858092</td>\n",
       "      <td>0.908662</td>\n",
       "      <td>0.780570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.182800</td>\n",
       "      <td>1.251818</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.914892</td>\n",
       "      <td>0.867247</td>\n",
       "      <td>0.882178</td>\n",
       "      <td>0.955797</td>\n",
       "      <td>0.956280</td>\n",
       "      <td>0.955532</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.949719</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.902388</td>\n",
       "      <td>0.803391</td>\n",
       "      <td>0.865761</td>\n",
       "      <td>0.902105</td>\n",
       "      <td>0.766436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.201800</td>\n",
       "      <td>1.254799</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.916183</td>\n",
       "      <td>0.866970</td>\n",
       "      <td>0.882416</td>\n",
       "      <td>0.955935</td>\n",
       "      <td>0.956398</td>\n",
       "      <td>0.955617</td>\n",
       "      <td>0.977352</td>\n",
       "      <td>0.951934</td>\n",
       "      <td>0.890270</td>\n",
       "      <td>0.904379</td>\n",
       "      <td>0.804080</td>\n",
       "      <td>0.865761</td>\n",
       "      <td>0.900395</td>\n",
       "      <td>0.765159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\k24034365\\.conda\\envs\\transformer\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 01:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test dataset: \n",
      "{'eval_loss': 1.252211570739746, 'eval_macro_acc': 0.9560778727445394, 'eval_macro_precision': 0.9073933692731901, 'eval_macro_recall': 0.8607660058002333, 'eval_macro_f1': 0.8727981196506881, 'eval_weighted_precision': 0.9554850126387042, 'eval_weighted_recall': 0.9560778727445394, 'eval_weighted_f1': 0.955216154636358, 'eval_sdoh_community_present_macro_f1': 0.9698462243349274, 'eval_sdoh_community_absent_macro_f1': 0.9267868730879762, 'eval_sdoh_education_macro_f1': 0.9033117042115573, 'eval_sdoh_economics_macro_f1': 0.9144219159207383, 'eval_sdoh_environment_macro_f1': 0.7741762648353204, 'eval_behavior_alcohol_macro_f1': 0.870937793832842, 'eval_behavior_tobacco_macro_f1': 0.9096333577625522, 'eval_behavior_drug_macro_f1': 0.713270823219591, 'eval_runtime': 73.8317, 'eval_samples_per_second': 14.262, 'eval_steps_per_second': 0.894, 'epoch': 15.0}\n"
     ]
    }
   ],
   "source": [
    "trainer2.train()\n",
    "\n",
    "test_metrics = trainer2.evaluate(test_ds)\n",
    "print(\"Performance on test dataset: \")\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b9499-0e68-4596-a059-3c5644e600c5",
   "metadata": {},
   "source": [
    "# Project Codes END, Thanks Very Much!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c91f1e-0d7f-4e8b-a19f-dabd368cbdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (transformer)",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
